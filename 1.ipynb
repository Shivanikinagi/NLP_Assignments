{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEuQ-vdepKvg",
        "outputId": "82a71007-3145-44d9-a85f-ad47e5f76808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (\n",
        "    WhitespaceTokenizer,\n",
        "    WordPunctTokenizer,\n",
        "    TreebankWordTokenizer,\n",
        "    TweetTokenizer,\n",
        "    MWETokenizer\n",
        ")\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "# Downloads required for lemmatization and specific tokenizers\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown foxes don't jump over the lazy dog! #nature @user\"\n",
        "\n",
        "# Whitespace Tokenizer\n",
        "ws_tokens = WhitespaceTokenizer().tokenize(text)\n",
        "\n",
        "# Punctuation-based (WordPunct)\n",
        "wp_tokens = WordPunctTokenizer().tokenize(text)\n",
        "\n",
        "# Treebank Tokenizer\n",
        "tb_tokens = TreebankWordTokenizer().tokenize(text)\n",
        "\n",
        "# Tweet Tokenizer\n",
        "tweet_tokens = TweetTokenizer().tokenize(text)\n",
        "\n",
        "# MWE Tokenizer (requires pre-defined expressions)\n",
        "mwe_tokenizer = MWETokenizer([('brown', 'foxes')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(tb_tokens) # Usually applied after initial tokenization"
      ],
      "metadata": {
        "id": "Lhu_IIcqpddr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "# Ensure the lemmatizer data is downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "words = [\"running\", \"flies\", \"happily\", \"better\"]\n",
        "\n",
        "# 1. Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "p_stems = [porter.stem(w) for w in words]\n",
        "\n",
        "# 2. Snowball Stemmer\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "s_stems = [snowball.stem(w) for w in words]\n",
        "\n",
        "# 3. Lemmatization (Verb context)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(w, pos='v') for w in words]\n",
        "\n",
        "# Printing the results\n",
        "print(f\"Original: {words}\")\n",
        "print(f\"Porter:   {p_stems}\")\n",
        "print(f\"Snowball: {s_stems}\")\n",
        "print(f\"Lemmas:   {lemmas}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlBpfP3IpzdS",
        "outputId": "b73196fe-baf2-4233-d10a-55d03f159f42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['running', 'flies', 'happily', 'better']\n",
            "Porter:   ['run', 'fli', 'happili', 'better']\n",
            "Snowball: ['run', 'fli', 'happili', 'better']\n",
            "Lemmas:   ['run', 'fly', 'happily', 'better']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6El3Dw6UqMlg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}